{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bU1SCVj6dyi1"
   },
   "source": [
    "\n",
    "  \n",
    "<h1><center><font size=10>Introduction to LLMs and GenAI</center></font></h1>\n",
    "<h1><center>Mini Project 1 : Basics of NLP: Text Cleaning & Vectorization</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjncuDf2qugI"
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9VSf2D_F5iU"
   },
   "source": [
    "### Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dense-medicaid"
   },
   "source": [
    "In today‚Äôs fast-paced e-commerce landscape, customer reviews significantly influence product perception and buying decisions. Businesses must actively monitor customer sentiment to extract insights and maintain a competitive edge. Ignoring negative feedback can lead to serious issues, such as:\n",
    "\n",
    "* Customer Churn: Unresolved complaints drive loyal customers away, reducing retention and future revenue.\n",
    "\n",
    "* Reputation Damage: Persistent negative sentiment can erode brand trust and deter new buyers.\n",
    "\n",
    "* Financial Loss: Declining sales and shifting customer preference toward competitors directly impact profitability.\n",
    "\n",
    "Actively tracking and addressing customer sentiment is essential for sustained growth and brand strength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ6qUJ3EqxW2"
   },
   "source": [
    "### Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkjASpFDeR4m"
   },
   "source": [
    "A growing e-commerce platform specializing in electronic gadgets collects customer feedback from product reviews, surveys, and social media. With a 200% increase in their customer base over three years and a recent 25% spike in feedback volume, their manual review process is no longer sustainable.\n",
    "\n",
    "To address this, the company aims to implement an AI-driven solution to automatically classify customer sentiments (positive, negative, or neutral).\n",
    "\n",
    "As a Data Scientist, your task is to analyze the provided customer reviews‚Äîalong with their labeled sentiments‚Äîand build a predictive model for sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saFx1pbT_zTP"
   },
   "source": [
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xA7JZQ9_2SO"
   },
   "source": [
    "- **Product ID**: An exclusive identification number for each product\n",
    "\n",
    "- **Product Review**: Insights and opinions shared by customers about the product\n",
    "\n",
    "- **Sentiment**: Sentiment associated with the product review, indicating whether the review expresses a positive, negative, or neutral sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqfNqaJCQeEE"
   },
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1753241900956,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "xhZUQ78gzw8H",
    "outputId": "abd06d8a-d683-4909-ec37-a71d8c43c885"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# to read and manipulate the data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_colwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)    \u001b[38;5;66;03m# setting column to the maximum column width as per the data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bhoomi\\Downloads\\jup\\Lib\\site-packages\\pandas\\__init__.py:31\u001b[0m\n\u001b[0;32m     28\u001b[0m         _missing_dependencies\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dependency\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _missing_dependencies:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to import required dependencies:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_missing_dependencies)\n\u001b[0;32m     33\u001b[0m     )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to import required dependencies:\nnumpy: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# to read and manipulate the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('max_colwidth', None)    # setting column to the maximum column width as per the data\n",
    "\n",
    "# to visualise data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# to use regular expressions for manipulating text data\n",
    "import re\n",
    "\n",
    "# to load the natural language toolkit\n",
    "# loading the wordnet module that is used in stemming\n",
    "\n",
    "# to remove common stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# to perform stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# to create Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# to split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to build a Random Forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# to compute metrics to evaluate the model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# To tune different models\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1753242240722,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "23wuOFYLYUcj",
    "outputId": "f5d02dbc-d911-479c-a009-f1f0a9fb09d0"
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('Product_Reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fantastic-rebel"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEvz0b9gzw8U"
   },
   "outputs": [],
   "source": [
    "# creating a copy of the data\n",
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvlzvKeqAH-i"
   },
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIH4md8nAL4v"
   },
   "source": [
    "### Checking the first five rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 164,
     "status": "ok",
     "timestamp": 1753242322190,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "bj4-QHJ6BiCS",
    "outputId": "9267dd76-80c1-444f-edd4-c4f0f6233e1d"
   },
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuWYF7W_AQx_"
   },
   "source": [
    "### Checking the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1753242999142,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "Mcb3m-xKzw8V",
    "outputId": "2f91f5ad-7148-4732-8ee7-7de82a56369b"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPbc-9HLs2WI"
   },
   "source": [
    "* The dataset has 1007 rows and 3 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBuO6NvsAT1k"
   },
   "source": [
    "### Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1753243043689,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "k0XZhWGRBiCV",
    "outputId": "42f29aed-89e0-4345-a934-1be62ffcd490"
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1i5McJGWBiCV"
   },
   "source": [
    "* There are no missing values in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZdNFg-5Zmiz"
   },
   "source": [
    "### Checking for duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1753243437873,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "gn5VDFNoBiCW",
    "outputId": "882e7525-bfae-4df9-b0b2-53a93c949346"
   },
   "outputs": [],
   "source": [
    "# checking for duplicate values\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwMJutR2BiCW"
   },
   "source": [
    "* There are 2 duplicate values in the dataset.\n",
    "* We'll drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1753243448656,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "3mgEIbCEcurU",
    "outputId": "12672bbf-6bfa-4256-ed78-f030138389bc"
   },
   "outputs": [],
   "source": [
    "# dropping duplicate values\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1753243694017,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "3VQpPykLdgsO",
    "outputId": "679bc939-9d01-4430-e3d8-6219bbbbe240"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUJ_B5KxhU3D"
   },
   "source": [
    "## Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akIOIRhfbvG8"
   },
   "source": [
    "#### Distribution of sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1753243917259,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "O5JSc0t9YI90",
    "outputId": "6ef37ce6-953a-467f-a016-ab530d7c57b8"
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=data, x=\"Sentiment\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1753244002631,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "ZYvGpRqf3ovu",
    "outputId": "d2bd403c-c5cf-4337-ef0f-6a2a824cf3bd"
   },
   "outputs": [],
   "source": [
    "data['Sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqnuCtG7bvHB"
   },
   "source": [
    "- Majority of the reviews are positive (\\~85%), followed by neutral reviews (8%), and then the positive reviews (\\~7%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wB_7p2rlkyd"
   },
   "source": [
    "# Recommended Metrics for this Case:\n",
    "| Metric                               | Why It's Important                                                                  |\n",
    "| ------------------------------------ | ----------------------------------------------------------------------------------- |\n",
    "| **Macro F1-Score**                   | Gives equal importance to all 3 classes regardless of imbalance.                    |\n",
    "| **Per-class Precision & Recall**     | Helps you understand how well the model detects **Neutral** and **Negative** cases. |\n",
    "| **Confusion Matrix**                 | Shows what types of mistakes your model is making.                                  |\n",
    "| *(Optional)* **ROC-AUC (per class)** | Can be helpful if you're using probabilistic outputs.                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-r5cv8iVlzIB"
   },
   "source": [
    "#### Macro F1 vs Micro F1\n",
    "| Type         | Use When                                              | What It Does                           |\n",
    "| ------------ | ----------------------------------------------------- | -------------------------------------- |\n",
    "| **Macro F1** | Treat all classes equally (class-balanced evaluation) | Averages F1 across all classes         |\n",
    "| **Micro F1** | Use when class sizes vary (class-imbalanced)          | Calculates global counts of TP, FP, FN |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4fvprg6u5fE"
   },
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJDPhhmvvxJ1"
   },
   "source": [
    "### Removing special characters from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGR2c6dCvT-Q"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# defining a function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    # Defining the regex pattern to match non-alphanumeric characters\n",
    "    pattern = '[^A-Za-z0-9]+'\n",
    "\n",
    "    # Finding the specified pattern and replacing non-alphanumeric characters with a blank string\n",
    "    new_text = ''.join(re.sub(pattern, ' ', text))\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQv4x8npctFZ"
   },
   "outputs": [],
   "source": [
    "# Applying the function to remove special characters\n",
    "data['cleaned_text'] = data['Product Review'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCxMsKChvT79"
   },
   "outputs": [],
   "source": [
    "# checking a couple of instances of cleaned data\n",
    "data.loc[0:3, ['Product Review','cleaned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpsuDWLnyJFw"
   },
   "source": [
    "- We can observe that the function removed the special characters and retained the alphabets and numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DftSZK9yQ74"
   },
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLOmMLSLRJT0"
   },
   "outputs": [],
   "source": [
    "# changing the case of the text data to lower case\n",
    "data['cleaned_text'] = data['cleaned_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P80CyzprdHH1"
   },
   "outputs": [],
   "source": [
    "# checking a couple of instances of cleaned data\n",
    "data.loc[0:3, ['Product Review','cleaned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF_kFCAxyg6L"
   },
   "source": [
    "- We can observe that all the text has now successfully been converted to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLoWwpxzylZH"
   },
   "source": [
    "### Removing extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjJN53m8RWCW"
   },
   "outputs": [],
   "source": [
    "# removing extra whitespaces from the text\n",
    "data['cleaned_text'] = data['cleaned_text'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCx3mBXiHRax"
   },
   "outputs": [],
   "source": [
    "# checking a couple of instances of cleaned data\n",
    "data.loc[0:3, ['Product Review','cleaned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwVOVENFz9fJ"
   },
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddYQH3ef0AUj"
   },
   "source": [
    "* The idea with stop word removal is to **exclude words that appear frequently throughout** all the documents in the corpus.\n",
    "* Pronouns and articles are typically categorized as stop words.\n",
    "* The `NLTK` library has an in-built list of stop words and it can utilize that list to remove the stop words from a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Run this once to download stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zof2x5co2X8g"
   },
   "outputs": [],
   "source": [
    "# defining a function to remove stop words using the NLTK library\n",
    "def remove_stopwords(text):\n",
    "    # Split text into separate words\n",
    "    words = text.split()\n",
    "\n",
    "    # Removing English language stopwords\n",
    "    new_text = ' '.join([word for word in words if word not in stopwords.words('english')])\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtmKZqDwfOlt"
   },
   "outputs": [],
   "source": [
    "# Applying the function to remove stop words using the NLTK library\n",
    "data['cleaned_text_without_stopwords'] = data['cleaned_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zikSkmBDfsu9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking a couple of instances of cleaned data\n",
    "data.loc[0:3,['cleaned_text','cleaned_text_without_stopwords']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0_cueGLW6Vt"
   },
   "source": [
    "* We observe that all the stopwords have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tN9S84Sj2om2"
   },
   "source": [
    "### Stemming/Lemmatization\n",
    "| Feature                       | **Stemming**                                              | **Lemmatization**                                                |\n",
    "| ----------------------------- | --------------------------------------------------------- | ---------------------------------------------------------------- |\n",
    "| **Definition**                | Removes suffixes to reduce words to root form             | Converts word to its **dictionary base form (lemma)**            |\n",
    "| **Output Example**            | ‚Äúrunning‚Äù ‚Üí **run**, ‚Äústudies‚Äù ‚Üí **studi**                | ‚Äúrunning‚Äù ‚Üí **run**, ‚Äústudies‚Äù ‚Üí **study**                       |\n",
    "| **Approach**                  | Rule-based truncation (chops off ends)                    | Dictionary + morphological analysis                              |\n",
    "| **Accuracy**                  | Lower (may produce non-words)                             | Higher (always valid words)                                      |\n",
    "| **Speed**                     | Fast (simpler rules)                                      | Slower (more complex processing)                                 |\n",
    "| **Tool Examples**             | `PorterStemmer`, `SnowballStemmer`                        | `WordNetLemmatizer`, `spaCy`                                     |\n",
    "| **Grammatical Understanding** | ‚ùå No                                                      | ‚úÖ Yes (considers part-of-speech)                                 |\n",
    "| **Language Dependency**       | Mostly English, rule-based                                | Requires proper linguistic resources                             |\n",
    "| **Use Case**                  | When speed is critical and precision isn‚Äôt (e.g., search) | When precision matters (e.g., text understanding, summarization) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTNdymFEKnSG"
   },
   "source": [
    "#### Summary:\n",
    "* Stemming: Quick, crude chopping of word ends. Fast but less accurate.\n",
    "* Lemmatization: Smart, linguistic reduction to base form. Slower but more accurate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wE5x7hyFDoks"
   },
   "source": [
    "# Comparision of Stemming, Lemmatization and POS(Part of Speech) Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQ_-OZrsDJnk"
   },
   "outputs": [],
   "source": [
    "# Stemming using NLTK (PorterStemmer)\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"studies\", \"flies\", \"easily\", \"happiness\"]\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word} ‚Üí {stemmer.stem(word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyV211QwDJ47"
   },
   "outputs": [],
   "source": [
    "# Lemmatization using NLTK (WordNetLemmatizer)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet #Downloads the WordNet lexical database.WordNet is adictionary-like database where Words are grouped into sets of synonyms\n",
    "import nltk\n",
    "\n",
    "# Make sure to download WordNet resources if not already done\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') # Open Multilingual WordNet package -This adds language translations, richer word forms, and improved morphological data to WordNet.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\", \"studies\", \"flies\", \"better\", \"happiness\"]\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word} ‚Üí {lemmatizer.lemmatize(word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNz-gjarDea8"
   },
   "outputs": [],
   "source": [
    "# Bonus: POS-aware Lemmatization with spaCy\n",
    "import spacy\n",
    "\n",
    "# Load English tokenizer, POS tagger, lemmatizer\n",
    "nlp = spacy.load(\"en_core_web_sm\") #Loads a pretrained English NLP model\n",
    "\n",
    "doc = nlp(\"running studies flies better happiness\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text} ({token.pos_}) ‚Üí {token.lemma_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urpKFz7C-D55"
   },
   "source": [
    "The Porter Stemmer is one of the widely-used algorithms for stemming, and it shorten words to their root form by removing suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2Nv5egY25SY"
   },
   "outputs": [],
   "source": [
    "# defining a function to perform stemming\n",
    "def apply_porter_stemmer(text):\n",
    "    # Split text into separate words\n",
    "    words = text.split()\n",
    "\n",
    "    # Applying the Porter Stemmer on every word of a message and joining the stemmed words back into a single string\n",
    "    new_text = ' '.join([ps.stem(word) for word in words])\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dR0rx6_2oCzY"
   },
   "outputs": [],
   "source": [
    "# Applying the function to perform stemming\n",
    "df ['final_cleaned_text'] = df ['cleaned_text_without_stopwords'].apply(apply_porter_stemmer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAc0gMU89KM7"
   },
   "outputs": [],
   "source": [
    "# checking a couple of instances of cleaned data\n",
    "data.loc[0:2,['cleaned_text_without_stopwords','final_cleaned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3F-popAzw8p"
   },
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6Xff8B8LSyL"
   },
   "source": [
    "* Text vectorization is the process of converting text into numerical format so that machine learning models can understand and work with it.\n",
    "\n",
    "* Since ML models can't work with raw text (like \"cat\", \"apple\", \"good\"), we transform the text into vectors (arrays of numbers) that represent words, sentences, or documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKfaJblQLekb"
   },
   "source": [
    "# Common Text Vectorization Methods\n",
    "| Method                                                 | Description                                                          | Library                   |\n",
    "| ------------------------------------------------------ | -------------------------------------------------------------------- | ------------------------- |\n",
    "| **Bag of Words (BoW using CountVectorizer)**                                 | Counts how many times each word appears in the document              | `sklearn`                 |\n",
    "| **TF-IDF (Term Frequency‚ÄìInverse Document Frequency)** | Adjusts BoW by down-weighting common words                           | `sklearn`                 |\n",
    "| **N-grams**                                            | Captures word combinations like bigrams/trigrams                     | `sklearn`                 |\n",
    "| **HashingVectorizer**                                  | Like BoW, but hashes tokens into fixed dimensions (memory-efficient) | `sklearn`                 |\n",
    "| **Word Embeddings (Word2Vec, GloVe)**                  | Maps words to dense, pretrained vectors with meaning                 | `nltk`, `gensim`, `spacy` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3IFOP2YU2ip"
   },
   "source": [
    "### 1. Bag of words (BOW using CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6QlX2WYzw8s"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initializing CountVectorizer with top 1000 words\n",
    "bow_vec = CountVectorizer(max_features = 1000)\n",
    "\n",
    "# Applying CountVectorizer on data\n",
    "data_features_BOW = bow_vec.fit_transform(data['final_cleaned_text'])\n",
    "\n",
    "# Convert the data features to array\n",
    "data_features_BOW = data_features_BOW.toarray()\n",
    "\n",
    "# Shape of the feature vector\n",
    "print(\"Shape of the feature vector\",data_features_BOW.shape)\n",
    "\n",
    "# Getting the 1000 words considered by the BoW model\n",
    "words = bow_vec.get_feature_names_out()\n",
    "\n",
    "print(\"first 10 words\",words[:10])\n",
    "print(\"last 10 words\",words[-10:])\n",
    "\n",
    "# Creating a DataFrame from the data features\n",
    "df_BOW = pd.DataFrame(data_features_BOW, columns=bow_vec.get_feature_names_out())\n",
    "df_BOW.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1M3x4zoG_bFM"
   },
   "source": [
    "- From the above dataframe, we can observe that the word *yet* is present only once in the third document, and the word *would* is presented twice in the fourth document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pqgH43PU-ym"
   },
   "source": [
    "### 2. TFIDFVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-VXf1CdSDj_"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initializing tfidf\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# Applying TfidfVectorizer on data\n",
    "data_features_tfidf = tfidf.fit_transform(data['final_cleaned_text'])\n",
    "\n",
    "# Convert the data features to array\n",
    "data_features_tfidf = data_features_tfidf.toarray()\n",
    "\n",
    "# Shape of the feature vector\n",
    "print(\"Shape of the feature vector\",data_features_tfidf.shape)\n",
    "\n",
    "# Getting the 1000 words considered by the BoW model\n",
    "words = tfidf.get_feature_names_out()\n",
    "\n",
    "print(\"first 10 words\",words[:10])\n",
    "print(\"last 10 words\",words[-10:])\n",
    "\n",
    "# Creating a DataFrame from the data features\n",
    "df_tfidf = pd.DataFrame(data_features_tfidf, columns=tfidf.get_feature_names_out())\n",
    "df_tfidf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkBPgJZAVGnO"
   },
   "source": [
    "### 3. n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipxybmbEVJmv"
   },
   "outputs": [],
   "source": [
    "# Initializing CountVectorizer with top 1000 words\n",
    "ngram = CountVectorizer(max_features = 1000,ngram_range=(1, 2))\n",
    "\n",
    "# Applying CountVectorizer on data\n",
    "data_features_ngram = ngram.fit_transform(data['final_cleaned_text'])\n",
    "\n",
    "# Convert the data features to array\n",
    "data_features_ngram = data_features_ngram.toarray()\n",
    "\n",
    "# Shape of the feature vector\n",
    "print(\"Shape of the feature vector\",data_features_ngram.shape)\n",
    "\n",
    "# Getting the 1000 words considered by the BoW model\n",
    "words = ngram.get_feature_names_out()\n",
    "\n",
    "print(\"first 10 words\",words[:10])\n",
    "print(\"last 10 words\",words[-10:])\n",
    "\n",
    "# Creating a DataFrame from the data features\n",
    "df_ngram = pd.DataFrame(data_features_ngram, columns=ngram.get_feature_names_out())\n",
    "df_ngram.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4lwYN5bYmHp"
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MJsXAQNErE3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Create a list of datasets and their labels\n",
    "vectorized_datasets = [\n",
    "    (\"BoW\", df_BOW),\n",
    "    (\"TF-IDF\", df_tfidf),\n",
    "    (\"N-gram\", df_ngram)\n",
    "]\n",
    "\n",
    "# Your target variable\n",
    "y = data['Sentiment']\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Loop over each dataset and train both classifiers\n",
    "for name, X in vectorized_datasets:\n",
    "    # Split data (80/20)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n",
    "    # Random Forest\n",
    "    rf_model = RandomForestClassifier(random_state=100)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_preds = rf_model.predict(X_test)\n",
    "    rf_f1 = f1_score(y_test, rf_preds, average='macro')\n",
    "    results.append((f\"RandomForest - {name}\", rf_f1, rf_model, X_test, y_test, rf_preds))\n",
    "\n",
    "    # Multinomial Naive Bayes\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    nb_preds = nb_model.predict(X_test)\n",
    "    nb_f1 = f1_score(y_test, nb_preds, average='macro')\n",
    "    results.append((f\"NaiveBayes - {name}\", nb_f1, nb_model, X_test, y_test, nb_preds))\n",
    "\n",
    "# Sort results by F1 score (descending)\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print all F1 scores\n",
    "print(\"\\nüìä Model Performance (Macro F1-scores):\\n\")\n",
    "for label, f1_score_val, _, _, _, _ in results:\n",
    "    print(f\"{label:30s}: Macro F1 = {f1_score_val:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxHB8BboE2vA"
   },
   "outputs": [],
   "source": [
    "# Best model\n",
    "best_model_label, best_f1, best_model, X_test_best, y_test_best, y_pred_best = results[0]\n",
    "\n",
    "print(f\"\\n‚úÖ Best Model: {best_model_label} (Macro F1 = {best_f1:.4f})\\n\")\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test_best, y_pred_best))\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "cm = confusion_matrix(y_test_best, y_pred_best, labels=best_model.classes_)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=best_model.classes_, yticklabels=best_model.classes_)\n",
    "plt.title(f\"Confusion Matrix: {best_model_label}\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4cByLTXv6H3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wH77U4ukBiCe"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aetabA3poIq"
   },
   "source": [
    " * Analyzed the distribution of customer sentiments.\n",
    "\n",
    " * Applied text preprocessing techniques to clean raw review data.\n",
    "\n",
    " * Vectorized the text using 3 different techniques and trained a Random Forest model & Naive Baye's Model\n",
    "\n",
    " * Achieved an macro F1 score of 0.57 on the test dataset.\n",
    "\n",
    " * Future improvements include hyperparameter tuning or trying alternative models for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEXtBjG_oYwj"
   },
   "source": [
    "### Recommendations:\n",
    "\n",
    "*   Use model predictions to identify customer concerns and take timely,\n",
    "targeted actions‚Äîreducing revenue loss and improving satisfaction.\n",
    "*   Leverage sentiment insights to refine marketing strategies:\n",
    "\n",
    "\n",
    "*   Showcase positive feedback in promotions to strengthen brand image.\n",
    "\n",
    "* Use neutral/negative feedback to guide inventory and operational\n",
    "\n",
    "decisions.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2Lw8akCLT9c"
   },
   "source": [
    "<font size=6 color='blue'>Thanks...</font>\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NJDPhhmvvxJ1",
    "2DftSZK9yQ74",
    "hLoWwpxzylZH"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
